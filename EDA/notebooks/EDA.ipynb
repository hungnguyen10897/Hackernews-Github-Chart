{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import pandas_gbq\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(developer): Set key_path to the path to the service account key\n",
    "#                  file.\n",
    "key_path = \"/home/jovyan/work/airflow-hackernews-github-365ae261a883.json\"\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    key_path,\n",
    "    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    ")\n",
    "\n",
    "pandas_gbq.context.credentials = credentials\n",
    "pandas_gbq.context.project = \"airflow-hackernews-github\"\n",
    "\n",
    "process_date = \"20200101\"\n",
    "process_date_dash = \"2020-01-01\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Githubarchive dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What type of event is most pupolar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    SELECT \n",
      "        type, COUNT(*) AS count_\n",
      "    \n",
      "    FROM `githubarchive.day.20200101` \n",
      "    GROUP BY 1\n",
      "    ORDER BY 2 DESC\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#What type of event is most popular\n",
    "sql = f\"\"\"\n",
    "    SELECT \n",
    "        type, COUNT(*) AS count_\n",
    "    \n",
    "    FROM `githubarchive.day.{process_date}`\n",
    "    GROUP BY 1\n",
    "    ORDER BY 2 DESC\n",
    "\"\"\"\n",
    "\n",
    "print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 14/14 [00:01<00:00, 12.40rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             type  count_\n",
      "0                       PushEvent  570350\n",
      "1                     CreateEvent  145556\n",
      "2                      WatchEvent   76352\n",
      "3                PullRequestEvent   72025\n",
      "4               IssueCommentEvent   49779\n",
      "5                     DeleteEvent   30266\n",
      "6                     IssuesEvent   27008\n",
      "7                       ForkEvent   24715\n",
      "8   PullRequestReviewCommentEvent    8609\n",
      "9                     GollumEvent    4434\n",
      "10                   ReleaseEvent    3920\n",
      "11                    PublicEvent    2574\n",
      "12             CommitCommentEvent    1984\n",
      "13                    MemberEvent    1508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = pandas_gbq.read_gbq(sql, dialect='standard')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 repos with the most comments in their issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    SELECT \n",
      "        repo.name,\n",
      "        COUNT(*) AS count_\n",
      "    FROM `githubarchive.day.20200101`\n",
      "    WHERE type = 'IssueCommentEvent'\n",
      "    GROUP BY 1\n",
      "    ORDER BY 2 DESC\n",
      "    LIMIT 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = f\"\"\"\n",
    "    SELECT \n",
    "        repo.name,\n",
    "        COUNT(*) AS count_\n",
    "    FROM `githubarchive.day.{process_date}`\n",
    "    WHERE type = 'IssueCommentEvent'\n",
    "    GROUP BY 1\n",
    "    ORDER BY 2 DESC\n",
    "    LIMIT 10\n",
    "\"\"\"\n",
    "print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 10/10 [00:01<00:00,  8.64rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        name  count_\n",
      "0               angband-import/angband-trac2    2915\n",
      "1                               hankcs/HanLP     563\n",
      "2             google-test/signcla-probe-repo     384\n",
      "3    test-organization-kkjeer/bot-validation     196\n",
      "4          test-organization-kkjeer/app-test     195\n",
      "5                          dotnet/docs.ja-jp     168\n",
      "6                             rust-lang/rust     164\n",
      "7                      kubernetes/kubernetes     161\n",
      "8                          pandas-dev/pandas     158\n",
      "9  bcbi-test/automerge-integration-test-repo     140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = pandas_gbq.read_gbq(sql, dialect = 'standard')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 repos by watch and fork events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    SELECT\n",
      "        repo.name,\n",
      "        SUM(IF(type='WatchEvent', 1, 0)) AS numOfWatchEvents,\n",
      "        SUM(IF(type='ForkEvent', 1, 0)) AS numOfForkEvents,\n",
      "        COUNT(*) AS count_\n",
      "    FROM `githubarchive.day.20200101`\n",
      "    WHERE type IN ('WatchEvent', 'ForkEvent')\n",
      "    GROUP BY 1\n",
      "    ORDER BY 2 DESC\n",
      "    LIMIT 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = f\"\"\"\n",
    "    SELECT\n",
    "        repo.name,\n",
    "        SUM(IF(type='WatchEvent', 1, 0)) AS numOfWatchEvents,\n",
    "        SUM(IF(type='ForkEvent', 1, 0)) AS numOfForkEvents,\n",
    "        COUNT(*) AS count_\n",
    "    FROM `githubarchive.day.{process_date}`\n",
    "    WHERE type IN ('WatchEvent', 'ForkEvent')\n",
    "    GROUP BY 1\n",
    "    ORDER BY 2 DESC\n",
    "    LIMIT 10\n",
    "\"\"\"\n",
    "print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 10/10 [00:01<00:00,  9.16rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  name  numOfWatchEvents  numOfForkEvents  \\\n",
      "0               victorqribeiro/isocity               615               35   \n",
      "1                      sw-yx/spark-joy               266                9   \n",
      "2                     imsnif/bandwhich               265               13   \n",
      "3             socialpoint-labs/sheetfu               236               10   \n",
      "4                   pingcap/chaos-mesh               220                6   \n",
      "5                         pakastin/car               219               15   \n",
      "6  feelschaotic/AndroidKnowledgeSystem               215               32   \n",
      "7                 testerSunshine/12306               210               83   \n",
      "8                     mayeaux/nodetube               195               13   \n",
      "9                 synesthesiam/rhasspy               175                3   \n",
      "\n",
      "   count_  \n",
      "0     650  \n",
      "1     275  \n",
      "2     278  \n",
      "3     246  \n",
      "4     226  \n",
      "5     234  \n",
      "6     247  \n",
      "7     293  \n",
      "8     208  \n",
      "9     178  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = pandas_gbq.read_gbq(sql, dialect = 'standard')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hackernews Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 domain ranked by average score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    SELECT \n",
      "        REGEXP_EXTRACT(url, '//([^/]*)/?') as domain,\n",
      "        AVG(score) AS average_score,\n",
      "        COUNT(*) AS count_\n",
      "    FROM `bigquery-public-data.hacker_news.full`\n",
      "    WHERE url != ''\n",
      "    GROUP BY 1\n",
      "    ORDER BY 3 DESC, 2 DESC\n",
      "    LIMIT 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "    SELECT \n",
    "        REGEXP_EXTRACT(url, '//([^/]*)/?') as domain,\n",
    "        AVG(score) AS average_score,\n",
    "        COUNT(*) AS count_\n",
    "    FROM `bigquery-public-data.hacker_news.full`\n",
    "    WHERE url != ''\n",
    "    GROUP BY 1\n",
    "    ORDER BY 3 DESC, 2 DESC\n",
    "    LIMIT 10\n",
    "\"\"\"\n",
    "print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 10/10 [00:01<00:00,  8.94rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                domain  average_score  count_\n",
      "0           medium.com       7.707072   94542\n",
      "1           github.com      16.416370   92240\n",
      "2      www.youtube.com       4.578248   67094\n",
      "3      www.nytimes.com      20.502744   47008\n",
      "4       techcrunch.com      17.591803   41017\n",
      "5      arstechnica.com      14.925032   27385\n",
      "6     en.wikipedia.org       9.789674   20763\n",
      "7  www.theguardian.com      17.142886   20016\n",
      "8        www.wired.com      13.698954   19698\n",
      "9    www.bloomberg.com      24.790660   19657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = pandas_gbq.read_gbq(sql, dialect = 'standard')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 domain ranked by number of scores greater than 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    SELECT \n",
      "        REGEXP_EXTRACT(url, '//([^/]*)/?') as domain,\n",
      "        SUM(IF(score>40, 1, 0)),\n",
      "        COUNT(*) AS count_\n",
      "    FROM `bigquery-public-data.hacker_news.full`\n",
      "    WHERE url != ''\n",
      "    GROUP BY 1\n",
      "    ORDER BY 2 DESC, 3 DESC\n",
      "    LIMIT 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "    SELECT \n",
    "        REGEXP_EXTRACT(url, '//([^/]*)/?') as domain,\n",
    "        SUM(IF(score>40, 1, 0)) num>40,\n",
    "        COUNT(*) AS count_\n",
    "    FROM `bigquery-public-data.hacker_news.full`\n",
    "    WHERE url != ''\n",
    "    GROUP BY 1\n",
    "    ORDER BY 2 DESC, 3 DESC\n",
    "    LIMIT 10\n",
    "\"\"\"\n",
    "print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 10/10 [00:01<00:00,  9.08rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   domain   f0_  count_\n",
      "0              github.com  8274   92240\n",
      "1         www.nytimes.com  5617   47008\n",
      "2          techcrunch.com  4067   41017\n",
      "3              medium.com  2881   94542\n",
      "4       www.bloomberg.com  2739   19657\n",
      "5         arstechnica.com  2311   27385\n",
      "6     www.theguardian.com  1867   20016\n",
      "7           www.wired.com  1597   19698\n",
      "8  www.washingtonpost.com  1541   13609\n",
      "9             www.bbc.com  1537   13310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = pandas_gbq.read_gbq(sql, dialect = 'standard')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Hackernews stories from Github by score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    SELECT \n",
      "        `by`,\n",
      "        id AS story_id,\n",
      "        REGEXP_EXTRACT(url, \"(https?://github.com/[^/]*/[^/#?]*)\") AS url,\n",
      "        SUM(score) AS score\n",
      "    FROM `bigquery-public-data.hacker_news.stories`\n",
      "    WHERE url LIKE '%https://github.com%'\n",
      "        AND url NOT LIKE '%github.com/blog/%'\n",
      "    GROUP BY 1,2,3\n",
      "    ORDER BY 4 DESC\n",
      "    LIMIT 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "    SELECT \n",
    "        `by`,\n",
    "        id AS story_id,\n",
    "        REGEXP_EXTRACT(url, \"(https?://github.com/[^/]*/[^/#?]*)\") AS url,\n",
    "        SUM(score) AS score\n",
    "    FROM `bigquery-public-data.hacker_news.stories`\n",
    "    WHERE url LIKE '%https://github.com%'\n",
    "        AND url NOT LIKE '%github.com/blog/%'\n",
    "    GROUP BY 1,2,3\n",
    "    ORDER BY 4 DESC\n",
    "    LIMIT 10\n",
    "\"\"\"\n",
    "print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 10/10 [00:01<00:00,  9.09rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            by  story_id                                                url  \\\n",
      "0  realfuncode  10101469     https://github.com/shadowsocks/shadowsocks-iOS   \n",
      "1   hannahmitt  10204018           https://github.com/HannahMitt/HomeMirror   \n",
      "2    peteretep   2661209                https://github.com/MrMEEE/bumblebee   \n",
      "3    peterhunt   9271246           https://github.com/facebook/react-native   \n",
      "4    timmorgan   8085213                https://github.com/churchio/onebody   \n",
      "5    codeulike   6048761                https://github.com/mame/quine-relay   \n",
      "6      tarruda   7278214                   https://github.com/neovim/neovim   \n",
      "7       chadrs   3287272  https://github.com/chadselph/OOptOut-Chrome-Ex...   \n",
      "8     onestone   9542267                      https://github.com/iojs/io.js   \n",
      "9    DaGardner   9196218                  https://github.com/mackyle/sqlite   \n",
      "\n",
      "   score  \n",
      "0   1543  \n",
      "1   1172  \n",
      "2   1123  \n",
      "3   1039  \n",
      "4   1020  \n",
      "5    983  \n",
      "6    879  \n",
      "7    820  \n",
      "8    794  \n",
      "9    792  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = pandas_gbq.read_gbq(sql, dialect = 'standard')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Final Table: GitHub on Hacker News Trends of 2020-01-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "WITH github_activity AS (\n",
      "    SELECT \n",
      "        repo.name AS repo,\n",
      "        CONCAT('https://github.com/', repo.name) AS url,\n",
      "        SUM(IF(type='WatchEvent', 1, 0)) AS numOfWatchEvents,\n",
      "        SUM(IF(type='ForkEvent', 1, 0)) AS numOfForkEvents,\n",
      "        COUNT(*) AS count_\n",
      "    FROM `githubarchive.day.20200101`\n",
      "    WHERE type IN ('WatchEvent', 'ForkEvent')\n",
      "    GROUP BY 1,2\n",
      "),\n",
      "hacker_news AS (\n",
      "    SELECT \n",
      "        EXTRACT(DATE FROM timestamp) AS date,\n",
      "        `by` AS submitter,\n",
      "        id AS story_id,\n",
      "        REGEXP_EXTRACT(url, \"(https?://github.com/[^/]*/[^/#?]*)\") AS url,\n",
      "        SUM(score) AS score\n",
      "    FROM `bigquery-public-data.hacker_news.full`\n",
      "    WHERE\n",
      "        type='story'\n",
      "        AND EXTRACT(DATE FROM timestamp)='2020-01-01'\n",
      "        AND url LIKE '%https://github.com%'\n",
      "        AND url NOT LIKE '%github.com/blog/%'\n",
      "    GROUP BY 1,2,3,4\n",
      ")\n",
      "\n",
      "SELECT \n",
      "    a.date AS date,\n",
      "    b.repo AS github_repo,\n",
      "    a.score AS score,\n",
      "    a.story_id AS story_id,\n",
      "    b.numOfWatchEvents AS numOfWatchEvents,\n",
      "    b.numOfForkEvents AS numOfForkEvents\n",
      "FROM hacker_news AS a\n",
      "LEFT JOIN github_activity AS b\n",
      "ON a.url = b.url\n",
      "ORDER BY score DESC\n",
      "LIMIT 10\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "\n",
    "WITH github_activity AS (\n",
    "    SELECT \n",
    "        repo.name AS repo,\n",
    "        CONCAT('https://github.com/', repo.name) AS url,\n",
    "        SUM(IF(type='WatchEvent', 1, 0)) AS numOfWatchEvents,\n",
    "        SUM(IF(type='ForkEvent', 1, 0)) AS numOfForkEvents,\n",
    "        COUNT(*) AS count_\n",
    "    FROM `githubarchive.day.20200101`\n",
    "    WHERE type IN ('WatchEvent', 'ForkEvent')\n",
    "    GROUP BY 1,2\n",
    "),\n",
    "hacker_news AS (\n",
    "    SELECT \n",
    "        EXTRACT(DATE FROM timestamp) AS date,\n",
    "        `by` AS submitter,\n",
    "        id AS story_id,\n",
    "        REGEXP_EXTRACT(url, \"(https?://github.com/[^/]*/[^/#?]*)\") AS url,\n",
    "        SUM(score) AS score\n",
    "    FROM `bigquery-public-data.hacker_news.full`\n",
    "    WHERE\n",
    "        type='story'\n",
    "        AND EXTRACT(DATE FROM timestamp)='2020-01-01'\n",
    "        AND url LIKE '%https://github.com%'\n",
    "        AND url NOT LIKE '%github.com/blog/%'\n",
    "    GROUP BY 1,2,3,4\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    a.date AS date,\n",
    "    b.repo AS github_repo,\n",
    "    a.score AS score,\n",
    "    a.story_id AS story_id,\n",
    "    b.numOfWatchEvents AS numOfWatchEvents,\n",
    "    b.numOfForkEvents AS numOfForkEvents\n",
    "FROM hacker_news AS a\n",
    "LEFT JOIN github_activity AS b\n",
    "ON a.url = b.url\n",
    "ORDER BY score DESC\n",
    "LIMIT 10\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 10/10 [00:01<00:00,  9.03rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date                github_repo  score  story_id  numOfWatchEvents  \\\n",
      "0 2020-01-01   socialpoint-labs/sheetfu    224  21928325             236.0   \n",
      "1 2020-01-01           mayeaux/nodetube    151  21926666             195.0   \n",
      "2 2020-01-01             falcon78/Vusic    102  21928211             107.0   \n",
      "3 2020-01-01         abhiTronix/vidgear     89  21926922             148.0   \n",
      "4 2020-01-01            chaskiq/chaskiq     32  21929676               9.0   \n",
      "5 2020-01-01            wassuphq/wassup     13  21931615               2.0   \n",
      "6 2020-01-01  alexellis/faas-containerd     10  21928642               1.0   \n",
      "7 2020-01-01             kyai/redis-cui      6  21926151               1.0   \n",
      "8 2020-01-01             spypunk/sponge      6  21926740               4.0   \n",
      "9 2020-01-01                       None      3  21927644               NaN   \n",
      "\n",
      "   numOfForkEvents  \n",
      "0             10.0  \n",
      "1             13.0  \n",
      "2             10.0  \n",
      "3              3.0  \n",
      "4              1.0  \n",
      "5              0.0  \n",
      "6              1.0  \n",
      "7              0.0  \n",
      "8              0.0  \n",
      "9              NaN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = pandas_gbq.read_gbq(sql, dialect = 'standard')\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
